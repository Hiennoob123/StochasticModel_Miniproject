{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hiennoob123/StochasticModel_Miniproject/blob/main/Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Import Library\n",
        "# =============================================================================\n",
        "!pip install simpy\n",
        "import simpy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsStvLX-T7Ir",
        "outputId": "83979d33-5e26-4ced-db94-d11510f8f099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpy in /usr/local/lib/python3.12/dist-packages (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ED Class Definition\n",
        "# =============================================================================\n",
        "class EDTriageSystem:\n",
        "    \"\"\"Emergency Department Triage System with Dynamic Staffing\"\"\"\n",
        "\n",
        "    def __init__(self, env, params):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "\n",
        "        # System state\n",
        "        self.current_staff = params['initial_staff']\n",
        "\n",
        "        self.queues = {1: [], 2: [], 3: []} # Queue state\n",
        "        self.in_service = {1: [], 2: [], 3: []}  # Store patient objects being served\n",
        "\n",
        "        # Statistics\n",
        "        self.stats = {\n",
        "            'arrivals': 0,\n",
        "            'arrivals_by_level': {1: 0, 2: 0, 3: 0},\n",
        "            'blocked': 0,\n",
        "            'blocked_by_level': {1: 0, 2: 0, 3: 0},\n",
        "            'completed': {1: 0, 2: 0, 3: 0},\n",
        "            'total_reward': 0,\n",
        "            'staff_changes': [],\n",
        "            'queue_history': [],\n",
        "            'service_history': [],\n",
        "            'reward_history': []\n",
        "        }\n",
        "\n",
        "        # Patient counter\n",
        "        self.patient_id = 0\n",
        "\n",
        "        # Events for logging\n",
        "        self.events = []\n",
        "\n",
        "        # All patients for visualization\n",
        "        self.all_patients = {}  # patient_id -> patient object\n",
        "\n",
        "    # Add event with timestamp\n",
        "    def log_event(self, message):\n",
        "        self.events.append((self.env.now, message))\n",
        "\n",
        "    # Return Total patients in queue\n",
        "    def get_total_queue_size(self):\n",
        "        return sum(len(q) for q in self.queues.values())\n",
        "\n",
        "    # Return Total patients in service\n",
        "    def get_total_in_service(self):\n",
        "        return sum(len(s) for s in self.in_service.values())\n",
        "\n",
        "    # Arrival process\n",
        "    def patient_arrival(self):\n",
        "        while True:\n",
        "            # Inter-arrival time (exponential)\n",
        "            yield self.env.timeout(np.random.exponential(1.0 / self.params['mu']))\n",
        "\n",
        "            # Assign triage level\n",
        "            rand = np.random.random()\n",
        "            if rand < 0.1:\n",
        "                level = 1  # Critical\n",
        "            elif rand < 0.4:\n",
        "                level = 2  # Urgent\n",
        "            else:\n",
        "                level = 3  # Non-urgent\n",
        "\n",
        "            self.patient_id += 1\n",
        "            self.stats['arrivals'] += 1\n",
        "            self.stats['arrivals_by_level'][level] += 1\n",
        "\n",
        "            # Create patient object\n",
        "            patient = {\n",
        "                'id': self.patient_id,\n",
        "                'level': level,\n",
        "                'arrival_time': self.env.now,\n",
        "                'service_start': None,\n",
        "                'completion_time': None,\n",
        "                'status': 'waiting'  # waiting, in_service, completed, blocked\n",
        "            }\n",
        "            self.all_patients[self.patient_id] = patient\n",
        "\n",
        "            # Check queue capacity\n",
        "            if self.get_total_queue_size() >= self.params['M']:\n",
        "                self.stats['blocked'] += 1\n",
        "                self.stats['blocked_by_level'][level] += 1\n",
        "                self.stats['total_reward'] -= self.params['cp'] # Reward reduced by cp\n",
        "                patient['status'] = 'blocked'\n",
        "                self.log_event(f\"Patient {self.patient_id} (Level {level}) BLOCKED\")\n",
        "            else:\n",
        "                self.queues[level].append(patient)\n",
        "                self.log_event(f\"Patient {self.patient_id} (Level {level}) arrived\")\n",
        "\n",
        "                # Try to assign to staff immediately\n",
        "                self.env.process(self.assign_patient_to_staff())\n",
        "\n",
        "    # Assign patient to free staff\n",
        "    def assign_patient_to_staff(self):\n",
        "        # While staff is free\n",
        "        while self.get_total_in_service() < self.current_staff:\n",
        "            assigned = False\n",
        "\n",
        "            # Find highest priority patient (Level 1 > 2 > 3)\n",
        "            for level in [1, 2, 3]:\n",
        "                if len(self.queues[level]) > 0:\n",
        "                    patient = self.queues[level].pop(0)\n",
        "                    patient['status'] = 'in_service'\n",
        "                    patient['service_start'] = self.env.now\n",
        "                    self.in_service[level].append(patient)\n",
        "\n",
        "                    self.log_event(f\"Patient {patient['id']} (Level {level}) started service\")\n",
        "\n",
        "                    # Start service process\n",
        "                    self.env.process(self.patient_service(patient))\n",
        "                    assigned = True\n",
        "                    break\n",
        "\n",
        "            # If no patients waiting, stop trying\n",
        "            if not assigned:\n",
        "                break\n",
        "\n",
        "        yield self.env.timeout(0)\n",
        "\n",
        "    def patient_service(self, patient):\n",
        "        \"\"\"Service a patient\"\"\"\n",
        "        level = patient['level']\n",
        "\n",
        "        # Service time (exponential)\n",
        "        service_rates = {\n",
        "            1: self.params['lambda1'],\n",
        "            2: self.params['lambda2'],\n",
        "            3: self.params['lambda3']\n",
        "        }\n",
        "        service_time = np.random.exponential(1.0 / service_rates[level])\n",
        "\n",
        "        yield self.env.timeout(service_time)\n",
        "\n",
        "        # Complete service\n",
        "        self.in_service[level].remove(patient)\n",
        "        patient['status'] = 'completed'\n",
        "        patient['completion_time'] = self.env.now\n",
        "        self.stats['completed'][level] += 1\n",
        "\n",
        "        # Add reward\n",
        "        rewards = {1: self.params['r1'], 2: self.params['r2'], 3: self.params['r3']}\n",
        "        self.stats['total_reward'] += rewards[level]\n",
        "\n",
        "        self.log_event(f\"Patient {patient['id']} (Level {level}) completed (+{rewards[level]})\")\n",
        "\n",
        "        # Try to assign next patient\n",
        "        self.env.process(self.assign_patient_to_staff())\n",
        "# =============================================================================\n",
        "# Computing Cost\n",
        "# =============================================================================\n",
        "    # Cost for maintenance\n",
        "    def maintenance_cost(self):\n",
        "        while True:\n",
        "            yield self.env.timeout(1.0)\n",
        "            cost = self.params['cm'] * self.current_staff\n",
        "            self.stats['total_reward'] -= cost\n",
        "            self.log_event(f\"Maintenance cost: -{cost}\")\n",
        "\n",
        "    # Record state every 0.1 hour\n",
        "    def record_state(self):\n",
        "        while True:\n",
        "            self.stats['queue_history'].append({\n",
        "                'time': self.env.now,\n",
        "                'q1': len(self.queues[1]),\n",
        "                'q2': len(self.queues[2]),\n",
        "                'q3': len(self.queues[3]),\n",
        "                'total_queue': self.get_total_queue_size()\n",
        "            })\n",
        "\n",
        "            self.stats['service_history'].append({\n",
        "                'time': self.env.now,\n",
        "                'b1': len(self.in_service[1]),\n",
        "                'b2': len(self.in_service[2]),\n",
        "                'b3': len(self.in_service[3]),\n",
        "                'total_service': self.get_total_in_service(),\n",
        "                'staff': self.current_staff\n",
        "            })\n",
        "\n",
        "            self.stats['reward_history'].append({\n",
        "                'time': self.env.now,\n",
        "                'reward': self.stats['total_reward']\n",
        "            })\n",
        "\n",
        "            yield self.env.timeout(0.1)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Staffing control: Changing staff level at time\n",
        "# =============================================================================\n",
        "    def change_staff(self, delta):\n",
        "        \"\"\"Change staff level\"\"\"\n",
        "        new_staff = max(0, min(self.params['Smax'], self.current_staff + delta))\n",
        "\n",
        "        if new_staff > self.current_staff:\n",
        "            # Add staff\n",
        "            increase = new_staff - self.current_staff\n",
        "            old_staff = self.current_staff\n",
        "            self.current_staff = new_staff\n",
        "\n",
        "            # Apply activation cost per new staff\n",
        "            self.stats['total_reward'] -= self.params['ca'] * increase\n",
        "            self.log_event(f\"Activated {increase} new staff (-{self.params['ca'] * increase} cost): {old_staff} → {new_staff}\")\n",
        "\n",
        "            # Try to assign waiting patients to newly available staff\n",
        "            for _ in range(increase):\n",
        "                self.env.process(self.assign_patient_to_staff())\n",
        "\n",
        "        elif new_staff < self.current_staff:\n",
        "            # Decrease staff\n",
        "            # NOTE: In real system, this would only take effect when staff finish service\n",
        "            # For now, we immediately reduce capacity (staff can finish current patients)\n",
        "            decrease = self.current_staff - new_staff\n",
        "            old_staff = self.current_staff\n",
        "            self.current_staff = new_staff\n",
        "            self.log_event(f\"Decreased staff by {decrease}: {old_staff} → {new_staff}\")\n",
        "\n",
        "        self.stats['staff_changes'].append({\n",
        "            'time': self.env.now,\n",
        "            'staff': self.current_staff,\n",
        "            'delta': delta\n",
        "        })\n",
        "\n",
        "        # Verify constraint: total in service should not exceed staff\n",
        "        total_in_service = self.get_total_in_service()\n",
        "        if total_in_service > self.current_staff:\n",
        "            self.log_event(f\"WARNING: {total_in_service} patients in service but only {self.current_staff} staff!\")\n",
        "\n",
        "    # Getting state\n",
        "    def get_state(self):\n",
        "        \"\"\"Get current MDP state: (Q1, Q2, Q3, B1, B2, B3, S)\"\"\"\n",
        "        return (\n",
        "            len(self.queues[1]),\n",
        "            len(self.queues[2]),\n",
        "            len(self.queues[3]),\n",
        "            len(self.in_service[1]),\n",
        "            len(self.in_service[2]),\n",
        "            len(self.in_service[3]),\n",
        "            self.current_staff\n",
        "        )\n",
        "\n",
        "    # Getting action space for current state\n",
        "    def get_valid_actions(self):\n",
        "        actions = [0]  # Always can do nothing\n",
        "\n",
        "        # Can increase by 1 if not at max\n",
        "        if self.current_staff < self.params['Smax']:\n",
        "            actions.append(1)\n",
        "\n",
        "        # Can decrease by any amount from -1 to -S_t\n",
        "        for decrease in range(1, self.current_staff + 1):\n",
        "            actions.append(-decrease)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    # Apply staffing policy\n",
        "    def staffing_policy(self, policy_type='static', rl_agent=None):\n",
        "        while True:\n",
        "            yield self.env.timeout(1.0)  # Decision every hour\n",
        "\n",
        "            # Static policy: Stays the same\n",
        "            if policy_type == 'static':\n",
        "                pass\n",
        "\n",
        "            # Threshold policy: Greedy based on total_queue\n",
        "            elif policy_type == 'threshold':\n",
        "                total_queue = self.get_total_queue_size()\n",
        "\n",
        "                if total_queue > self.params['M'] * 0.7 and self.current_staff < self.params['Smax']:\n",
        "                    self.change_staff(1)\n",
        "                elif total_queue < self.params['M'] * 0.3 and self.current_staff > 1:\n",
        "                    self.change_staff(-1)\n",
        "\n",
        "            # Reactive policy: Prioritize critical patient\n",
        "            elif policy_type == 'reactive':\n",
        "                critical_queue = len(self.queues[1])\n",
        "                total_queue = self.get_total_queue_size()\n",
        "\n",
        "                if critical_queue > 2 or total_queue > self.params['M'] * 0.8:\n",
        "                    if self.current_staff < self.params['Smax']:\n",
        "                        self.change_staff(1)\n",
        "                elif total_queue < 5 and critical_queue == 0:\n",
        "                    if self.current_staff > 2:\n",
        "                        self.change_staff(-1)\n",
        "\n",
        "            # RL policy\n",
        "            elif policy_type == 'rl' and rl_agent is not None:\n",
        "                # Get current state\n",
        "                state = self.get_state()\n",
        "                valid_actions = self.get_valid_actions()\n",
        "\n",
        "                # Get action from RL agent\n",
        "                action = rl_agent.select_action(state, valid_actions)\n",
        "\n",
        "                # Apply action\n",
        "                if action != 0:\n",
        "                    self.change_staff(action)"
      ],
      "metadata": {
        "id": "fWlklGYoUJp1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Visualize the simulation\n",
        "# =============================================================================\n",
        "def live_visual_simulation(params, sim_time=50, policy='reactive', rl_agent=None):\n",
        "    from matplotlib.animation import FuncAnimation\n",
        "    from IPython.display import HTML\n",
        "\n",
        "    env = simpy.Environment()\n",
        "    system = EDTriageSystem(env, params)\n",
        "\n",
        "    # Start processes\n",
        "    env.process(system.patient_arrival())\n",
        "    env.process(system.maintenance_cost())\n",
        "    env.process(system.record_state())\n",
        "    env.process(system.staffing_policy(policy, rl_agent))\n",
        "\n",
        "    # Setup figure\n",
        "    fig = plt.figure(figsize=(16, 10))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Main visualization area - ED Layout\n",
        "    ax_main = fig.add_subplot(gs[:2, :2])\n",
        "\n",
        "    # Statistics plots\n",
        "    ax_queue = fig.add_subplot(gs[0, 2])\n",
        "    ax_reward = fig.add_subplot(gs[1, 2])\n",
        "    ax_stats = fig.add_subplot(gs[2, :])\n",
        "\n",
        "    # Colors for triage levels\n",
        "    colors = {1: 'red', 2: 'orange', 3: 'gold'}\n",
        "\n",
        "    def draw_ed_layout(ax):\n",
        "        \"\"\"Draw the ED layout background\"\"\"\n",
        "        ax.clear()\n",
        "        ax.set_xlim(0, 10)\n",
        "        ax.set_ylim(0, 10)\n",
        "        ax.set_aspect('equal')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Title\n",
        "        ax.text(5, 9.5, 'Emergency Department - Live Simulation',\n",
        "                ha='center', fontsize=14, fontweight='bold')\n",
        "        ax.text(5, 9, f'Time: {env.now:.1f}h | Policy: {policy} | Staff: {system.current_staff}/{params[\"Smax\"]}',\n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "        # Entrance\n",
        "        entrance = mpatches.Rectangle((0, 7), 1, 2, linewidth=2,\n",
        "                                     edgecolor='black', facecolor='lightblue', alpha=0.3)\n",
        "        ax.add_patch(entrance)\n",
        "        ax.text(0.5, 8, 'ENTRANCE', ha='center', va='center', fontsize=8, fontweight='bold')\n",
        "\n",
        "        # Queue areas\n",
        "        queue_y_positions = {1: 7.5, 2: 5.5, 3: 3.5}\n",
        "        queue_labels = {1: 'CRITICAL', 2: 'URGENT', 3: 'NON-URGENT'}\n",
        "\n",
        "        for level in [1, 2, 3]:\n",
        "            y_pos = queue_y_positions[level]\n",
        "            queue_box = mpatches.Rectangle((1.5, y_pos-0.4), 4, 0.8, linewidth=2,\n",
        "                                          edgecolor=colors[level], facecolor=colors[level], alpha=0.1)\n",
        "            ax.add_patch(queue_box)\n",
        "            ax.text(1.2, y_pos, f'L{level}:', ha='right', va='center',\n",
        "                   fontsize=9, fontweight='bold', color=colors[level])\n",
        "            ax.text(3.5, y_pos+0.5, queue_labels[level], ha='center', va='bottom',\n",
        "                   fontsize=8, color=colors[level], fontweight='bold')\n",
        "\n",
        "        # Treatment area\n",
        "        treatment = mpatches.Rectangle((6.5, 2.5), 3, 5.5, linewidth=2,\n",
        "                                      edgecolor='green', facecolor='lightgreen', alpha=0.2)\n",
        "        ax.add_patch(treatment)\n",
        "        ax.text(8, 7.5, 'TREATMENT AREA', ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        # Staff positions (beds)\n",
        "        staff_positions = []\n",
        "        for i in range(params['Smax']):\n",
        "            row = i // 3\n",
        "            col = i % 3\n",
        "            x = 7 + col * 0.8\n",
        "            y = 6.5 - row * 0.8\n",
        "            staff_positions.append((x, y))\n",
        "\n",
        "            # Draw bed\n",
        "            if i < system.current_staff:\n",
        "                bed = mpatches.Circle((x, y), 0.15, color='green', alpha=0.3)\n",
        "            else:\n",
        "                bed = mpatches.Circle((x, y), 0.15, color='gray', alpha=0.2)\n",
        "            ax.add_patch(bed)\n",
        "\n",
        "        return staff_positions, queue_y_positions\n",
        "\n",
        "    def draw_patients(ax, staff_positions, queue_y_positions):\n",
        "        \"\"\"Draw patients as colored dots\"\"\"\n",
        "        # Draw queue patients\n",
        "        for level in [1, 2, 3]:\n",
        "            y_pos = queue_y_positions[level]\n",
        "            queue = system.queues[level]\n",
        "\n",
        "            for idx, patient in enumerate(queue):\n",
        "                x = 2 + (idx % 10) * 0.35\n",
        "                y = y_pos + (idx // 10) * 0.15\n",
        "\n",
        "                circle = mpatches.Circle((x, y), 0.12, color=colors[level],\n",
        "                                        edgecolor='black', linewidth=1, zorder=10)\n",
        "                ax.add_patch(circle)\n",
        "\n",
        "                # Patient ID\n",
        "                ax.text(x, y, str(patient['id']), ha='center', va='center',\n",
        "                       fontsize=6, color='white', fontweight='bold', zorder=11)\n",
        "\n",
        "        # Draw patients in service\n",
        "        service_idx = 0\n",
        "        for level in [1, 2, 3]:\n",
        "            for patient in system.in_service[level]:\n",
        "                if service_idx < len(staff_positions):\n",
        "                    x, y = staff_positions[service_idx]\n",
        "\n",
        "                    circle = mpatches.Circle((x, y), 0.15, color=colors[level],\n",
        "                                           edgecolor='darkgreen', linewidth=2, zorder=10)\n",
        "                    ax.add_patch(circle)\n",
        "\n",
        "                    ax.text(x, y, str(patient['id']), ha='center', va='center',\n",
        "                           fontsize=7, color='white', fontweight='bold', zorder=11)\n",
        "\n",
        "                    service_idx += 1\n",
        "\n",
        "        # Legend\n",
        "        legend_elements = [\n",
        "            mpatches.Patch(color='red', label='Level 1 (Critical)'),\n",
        "            mpatches.Patch(color='orange', label='Level 2 (Urgent)'),\n",
        "            mpatches.Patch(color='gold', label='Level 3 (Non-urgent)')\n",
        "        ]\n",
        "        ax.legend(handles=legend_elements, loc='lower left', fontsize=8)\n",
        "\n",
        "    def update_statistics_plots():\n",
        "        \"\"\"Update the statistics plots\"\"\"\n",
        "        if len(system.stats['queue_history']) < 2:\n",
        "            return\n",
        "\n",
        "        df_queue = pd.DataFrame(system.stats['queue_history'])\n",
        "        df_reward = pd.DataFrame(system.stats['reward_history'])\n",
        "\n",
        "        # Queue plot\n",
        "        ax_queue.clear()\n",
        "        ax_queue.plot(df_queue['time'], df_queue['q1'], 'r-', linewidth=2, label='L1')\n",
        "        ax_queue.plot(df_queue['time'], df_queue['q2'], color='orange', linewidth=2, label='L2')\n",
        "        ax_queue.plot(df_queue['time'], df_queue['q3'], color='gold', linewidth=2, label='L3')\n",
        "        ax_queue.axhline(y=params['M'], color='red', linestyle=':', label='Capacity')\n",
        "        ax_queue.set_xlabel('Time (h)', fontsize=8)\n",
        "        ax_queue.set_ylabel('Queue Size', fontsize=8)\n",
        "        ax_queue.set_title('Queue Over Time', fontsize=9, fontweight='bold')\n",
        "        ax_queue.legend(fontsize=7)\n",
        "        ax_queue.grid(True, alpha=0.3)\n",
        "        ax_queue.tick_params(labelsize=7)\n",
        "\n",
        "        # Reward plot\n",
        "        ax_reward.clear()\n",
        "        ax_reward.plot(df_reward['time'], df_reward['reward'], 'purple', linewidth=2)\n",
        "        ax_reward.set_xlabel('Time (h)', fontsize=8)\n",
        "        ax_reward.set_ylabel('Reward', fontsize=8)\n",
        "        ax_reward.set_title('Cumulative Reward', fontsize=9, fontweight='bold')\n",
        "        ax_reward.grid(True, alpha=0.3)\n",
        "        ax_reward.tick_params(labelsize=7)\n",
        "\n",
        "        # Statistics bar\n",
        "        ax_stats.clear()\n",
        "        stats_labels = ['Arrivals', 'In Queue', 'In Service', 'Completed', 'Blocked']\n",
        "        stats_values = [\n",
        "            system.stats['arrivals'],\n",
        "            system.get_total_queue_size(),\n",
        "            system.get_total_in_service(),\n",
        "            sum(system.stats['completed'].values()),\n",
        "            system.stats['blocked']\n",
        "        ]\n",
        "        stats_colors = ['blue', 'orange', 'green', 'lightgreen', 'red']\n",
        "\n",
        "        bars = ax_stats.bar(stats_labels, stats_values, color=stats_colors, alpha=0.7, edgecolor='black')\n",
        "        ax_stats.set_ylabel('Count', fontsize=9)\n",
        "        ax_stats.set_title(f'Statistics | Reward: {system.stats[\"total_reward\"]:.2f}',\n",
        "                          fontsize=10, fontweight='bold')\n",
        "        ax_stats.grid(True, alpha=0.3, axis='y')\n",
        "        ax_stats.tick_params(labelsize=8)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax_stats.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                         f'{int(height)}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "    def animate(frame):\n",
        "        \"\"\"Animation function\"\"\"\n",
        "        # Run simulation for a small time step\n",
        "        env.run(until=env.now + 0.2)\n",
        "\n",
        "        # Redraw everything\n",
        "        staff_positions, queue_y_positions = draw_ed_layout(ax_main)\n",
        "        draw_patients(ax_main, staff_positions, queue_y_positions)\n",
        "        update_statistics_plots()\n",
        "\n",
        "        return ax_main, ax_queue, ax_reward, ax_stats\n",
        "\n",
        "    # Initial draw\n",
        "    staff_positions, queue_y_positions = draw_ed_layout(ax_main)\n",
        "\n",
        "    # Create animation\n",
        "    frames = int(sim_time / 0.2)\n",
        "    anim = FuncAnimation(fig, animate, frames=frames, interval=200, blit=False, repeat=False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return anim, system\n"
      ],
      "metadata": {
        "id": "jwaMshfobxxe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Q-Learning agent\n",
        "# =============================================================================\n",
        "class QLearningAgent:\n",
        "    def __init__(self, params, learning_rate=0.1, discount_factor=0.95, epsilon=1):\n",
        "        self.params = params\n",
        "        self.alpha = learning_rate  # Learning rate\n",
        "        self.gamma = discount_factor  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "\n",
        "        # Q-table: Q(state, action) -> value\n",
        "        self.q_table = {}\n",
        "\n",
        "        # For tracking\n",
        "        self.episode_rewards = []\n",
        "        self.episode_states = []\n",
        "        self.episode_actions = []\n",
        "        self.episode_number = 0\n",
        "\n",
        "    # Updating decay_epsilon for episode = 10000\n",
        "    def decay_epsilon(self, episode):\n",
        "        self.epsilon = max(0.1, 1.0 - 0.000099 * episode)\n",
        "\n",
        "    # Get Q value\n",
        "    def get_q_value(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "    # Select action based on epsilon-greedy policy\n",
        "    def select_action(self, state, valid_actions, training=True):\n",
        "        # Update episode\n",
        "        self.episode_number += 1\n",
        "        self.decay_epsilon(self.episode_number)\n",
        "\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            # Explore: random action\n",
        "            return np.random.choice(valid_actions)\n",
        "        else:\n",
        "            # Exploit: best action\n",
        "            q_values = [self.get_q_value(state, a) for a in valid_actions]\n",
        "            max_q = max(q_values)\n",
        "\n",
        "            # Handle ties randomly\n",
        "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "\n",
        "    # Update Q-table\n",
        "    def update_q_value(self, state, action, reward, next_state, next_valid_actions):\n",
        "        current_q = self.get_q_value(state, action)\n",
        "\n",
        "        # Get max Q-value for next state\n",
        "        if next_valid_actions:\n",
        "            next_q_values = [self.get_q_value(next_state, a) for a in next_valid_actions]\n",
        "            max_next_q = max(next_q_values)\n",
        "        else:\n",
        "            max_next_q = 0.0\n",
        "\n",
        "        # Q-learning update\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
        "        self.q_table[(state, action)] = new_q\n",
        "\n",
        "    # Get policy from Q-table\n",
        "    def get_policy(self):\n",
        "        policy = {}\n",
        "\n",
        "        for (state, action), q_value in self.q_table.items():\n",
        "            if state not in policy or q_value > policy[state][1]:\n",
        "                policy[state] = (action, q_value)\n",
        "\n",
        "        return policy\n",
        "\n",
        "    # Save Q-table from file\n",
        "    def save_policy(self, filename):\n",
        "        import pickle\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.q_table, f)\n",
        "\n",
        "    # Load Q-table from file\n",
        "    def load_policy(self, filename):\n",
        "        import pickle\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.q_table = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "jQ-zJwBhb4hh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "odapqs6JOHBK",
        "outputId": "201e1ad2-29aa-4d78-a1c7-e9830e85dfee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SIMULATION-BASED REINFORCEMENT LEARNING\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "COMPARING ALL POLICIES\n",
            "======================================================================\n",
            "\n",
            "Training RL agent...\n",
            "\n",
            "======================================================================\n",
            "TRAINING RL AGENT\n",
            "Episodes: 10000 | Episode Length: 50h\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'QLearningAgent' object has no attribute 'epsilon_min'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3497391688.py\u001b[0m in \u001b[0;36mrl_control_process\u001b[0;34m(self, env, system, episode_length)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Select and execute action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3085717018.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, valid_actions, training)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3085717018.py\u001b[0m in \u001b[0;36mdecay_epsilon\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecay_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.000099\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'QLearningAgent' object has no attribute 'epsilon_min'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3497391688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SIMULATION-BASED REINFORCEMENT LEARNING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_all_policies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# ========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3497391688.py\u001b[0m in \u001b[0;36mcompare_all_policies\u001b[0;34m(params, sim_time)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulationBasedRLTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_training_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3497391688.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_episodes, episode_length, verbose_interval)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3497391688.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, episode_length, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Run simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simpy/core.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, until)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopSimulation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# == until.value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simpy/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntil\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSimTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'QLearningAgent' object has no attribute 'epsilon_min'"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Training the Simulation-based RL\n",
        "# =============================================================================\n",
        "class SimulationBasedRLTrainer:\n",
        "    def __init__(self, params, agent):\n",
        "        self.params = params\n",
        "        self.agent = agent\n",
        "        self.training_history = []\n",
        "\n",
        "    def run_episode(self, episode_length=50, verbose=False):\n",
        "        \"\"\"Run one training episode\"\"\"\n",
        "        env = simpy.Environment()\n",
        "        system = EDTriageSystem(env, self.params)\n",
        "\n",
        "        # Start basic processes (no staffing policy yet)\n",
        "        env.process(system.patient_arrival())\n",
        "        env.process(system.maintenance_cost())\n",
        "        env.process(system.record_state())\n",
        "\n",
        "        # Manual RL control process\n",
        "        env.process(self.rl_control_process(env, system, episode_length))\n",
        "\n",
        "        # Run simulation\n",
        "        env.run(until=episode_length)\n",
        "\n",
        "        total_reward = system.stats['total_reward']\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Episode completed: Reward = {total_reward:.2f}, \"\n",
        "                  f\"Arrivals = {system.stats['arrivals']}, \"\n",
        "                  f\"Blocked = {system.stats['blocked']}, \"\n",
        "                  f\"Completed = {sum(system.stats['completed'].values())}\")\n",
        "\n",
        "        return total_reward, system\n",
        "\n",
        "    def rl_control_process(self, env, system, episode_length):\n",
        "        \"\"\"Process for RL agent to make decisions\"\"\"\n",
        "        prev_state = None\n",
        "        prev_action = None\n",
        "        prev_reward = 0\n",
        "\n",
        "        while env.now < episode_length:\n",
        "            yield env.timeout(1.0)  # Decision every hour\n",
        "\n",
        "            # Get current state\n",
        "            current_state = system.get_state()\n",
        "            valid_actions = system.get_valid_actions()\n",
        "\n",
        "            # Calculate reward from last step\n",
        "            current_reward = system.stats['total_reward']\n",
        "            step_reward = current_reward - prev_reward\n",
        "\n",
        "            # Update Q-value if we have a previous transition\n",
        "            if prev_state is not None:\n",
        "                self.agent.update_q_value(\n",
        "                    prev_state, prev_action, step_reward,\n",
        "                    current_state, valid_actions\n",
        "                )\n",
        "\n",
        "            # Select and execute action\n",
        "            action = self.agent.select_action(current_state, valid_actions, training=True)\n",
        "\n",
        "            if action != 0:\n",
        "                system.change_staff(action)\n",
        "\n",
        "            # Store for next update\n",
        "            prev_state = current_state\n",
        "            prev_action = action\n",
        "            prev_reward = current_reward\n",
        "\n",
        "    def train(self, num_episodes=100, episode_length=50, verbose_interval=10):\n",
        "        \"\"\"Train the agent over multiple episodes\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"TRAINING RL AGENT\")\n",
        "        print(f\"Episodes: {num_episodes} | Episode Length: {episode_length}h\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        episode_rewards = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            reward, system = self.run_episode(episode_length, verbose=False)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            self.training_history.append({\n",
        "                'episode': episode,\n",
        "                'reward': reward,\n",
        "                'arrivals': system.stats['arrivals'],\n",
        "                'blocked': system.stats['blocked'],\n",
        "                'completed': sum(system.stats['completed'].values()),\n",
        "                'q_table_size': len(self.agent.q_table)\n",
        "            })\n",
        "\n",
        "            if (episode + 1) % verbose_interval == 0:\n",
        "                avg_reward = np.mean(episode_rewards[-verbose_interval:])\n",
        "                avg_blocked = np.mean([h['blocked'] for h in self.training_history[-verbose_interval:]])\n",
        "                print(f\"Episode {episode+1}/{num_episodes} | \"\n",
        "                      f\"Avg Reward: {avg_reward:.2f} | \"\n",
        "                      f\"Avg Blocked: {avg_blocked:.1f} | \"\n",
        "                      f\"Q-table: {len(self.agent.q_table)}\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"TRAINING COMPLETE\")\n",
        "        print(f\"Final Q-table size: {len(self.agent.q_table)}\")\n",
        "        print(f\"Average reward (last 10 episodes): {np.mean(episode_rewards[-10:]):.2f}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return episode_rewards\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        \"\"\"Plot training progress\"\"\"\n",
        "        df = pd.DataFrame(self.training_history)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        # Reward over episodes\n",
        "        axes[0, 0].plot(df['episode'], df['reward'], linewidth=2, alpha=0.6)\n",
        "        # Moving average\n",
        "        window = min(10, len(df) // 10)\n",
        "        if window > 0:\n",
        "            moving_avg = df['reward'].rolling(window=window).mean()\n",
        "            axes[0, 0].plot(df['episode'], moving_avg, 'r-', linewidth=2, label=f'Moving Avg ({window})')\n",
        "        axes[0, 0].set_xlabel('Episode')\n",
        "        axes[0, 0].set_ylabel('Total Reward')\n",
        "        axes[0, 0].set_title('Learning Progress - Reward', fontweight='bold')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Blocked patients\n",
        "        axes[0, 1].plot(df['episode'], df['blocked'], 'r-', linewidth=2, alpha=0.6)\n",
        "        if window > 0:\n",
        "            moving_avg = df['blocked'].rolling(window=window).mean()\n",
        "            axes[0, 1].plot(df['episode'], moving_avg, 'darkred', linewidth=2, label=f'Moving Avg ({window})')\n",
        "        axes[0, 1].set_xlabel('Episode')\n",
        "        axes[0, 1].set_ylabel('Blocked Patients')\n",
        "        axes[0, 1].set_title('Blocked Patients Over Time', fontweight='bold')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Completed services\n",
        "        axes[1, 0].plot(df['episode'], df['completed'], 'g-', linewidth=2, alpha=0.6)\n",
        "        if window > 0:\n",
        "            moving_avg = df['completed'].rolling(window=window).mean()\n",
        "            axes[1, 0].plot(df['episode'], moving_avg, 'darkgreen', linewidth=2, label=f'Moving Avg ({window})')\n",
        "        axes[1, 0].set_xlabel('Episode')\n",
        "        axes[1, 0].set_ylabel('Completed Services')\n",
        "        axes[1, 0].set_title('Completed Services Over Time', fontweight='bold')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Q-table growth\n",
        "        axes[1, 1].plot(df['episode'], df['q_table_size'], 'purple', linewidth=2)\n",
        "        axes[1, 1].set_xlabel('Episode')\n",
        "        axes[1, 1].set_ylabel('Q-table Size')\n",
        "        axes[1, 1].set_title('Q-table Growth', fontweight='bold')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# BLOCK 9: Policy comparison wrapper\n",
        "# Description:\n",
        "#   High-level function to train the RL agent, evaluate all policies (static,\n",
        "#   threshold, reactive, RL), print summary metrics, and visualize comparisons.\n",
        "# =============================================================================\n",
        "def compare_all_policies(params, sim_time=100):\n",
        "    \"\"\"Compare all policies including RL\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"COMPARING ALL POLICIES\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Train RL agent first\n",
        "    print(\"Training RL agent...\")\n",
        "    agent = QLearningAgent(params, learning_rate=0.1, discount_factor=0.95, epsilon=0.2)\n",
        "    trainer = SimulationBasedRLTrainer(params, agent)\n",
        "    episode_rewards = trainer.train(num_episodes=10000, episode_length=50, verbose_interval=20)\n",
        "    trainer.plot_training_progress()\n",
        "\n",
        "    # Set to exploitation mode\n",
        "    agent.epsilon = 0.0  # No exploration during evaluation\n",
        "\n",
        "    policies = {\n",
        "        'static': None,\n",
        "        'threshold': None,\n",
        "        'reactive': None,\n",
        "        'rl': agent\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"EVALUATING POLICIES\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    for policy_name, rl_agent in policies.items():\n",
        "        # Run simulation\n",
        "        env = simpy.Environment()\n",
        "        system = EDTriageSystem(env, params)\n",
        "\n",
        "        env.process(system.patient_arrival())\n",
        "        env.process(system.maintenance_cost())\n",
        "        env.process(system.record_state())\n",
        "        env.process(system.staffing_policy(policy_name if policy_name != 'rl' else 'rl', rl_agent))\n",
        "\n",
        "        env.run(until=sim_time)\n",
        "\n",
        "        results[policy_name] = {\n",
        "            'total_reward': system.stats['total_reward'],\n",
        "            'completed': sum(system.stats['completed'].values()),\n",
        "            'blocked': system.stats['blocked'],\n",
        "            'blocking_rate': system.stats['blocked']/max(1, system.stats['arrivals'])*100,\n",
        "            'avg_queue': np.mean([h['total_queue'] for h in system.stats['queue_history']]) if system.stats['queue_history'] else 0,\n",
        "            'avg_staff': np.mean([h['staff'] for h in system.stats['service_history']]) if system.stats['service_history'] else params['initial_staff']\n",
        "        }\n",
        "\n",
        "        print(f\"Policy: {policy_name:12s} | Reward: {results[policy_name]['total_reward']:8.2f} | \"\n",
        "              f\"Completed: {results[policy_name]['completed']:4d} | \"\n",
        "              f\"Blocked: {results[policy_name]['blocked']:3d} ({results[policy_name]['blocking_rate']:.1f}%)\")\n",
        "\n",
        "    # Plot comparison\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('Policy Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "    policy_names = list(results.keys())\n",
        "    colors_map = {'static': 'gray', 'threshold': 'blue', 'reactive': 'green', 'rl': 'red'}\n",
        "    colors = [colors_map[p] for p in policy_names]\n",
        "\n",
        "    # Total reward\n",
        "    axes[0, 0].bar(policy_names, [results[p]['total_reward'] for p in policy_names], color=colors, alpha=0.7, edgecolor='black')\n",
        "    axes[0, 0].set_ylabel('Total Reward')\n",
        "    axes[0, 0].set_title('Total Reward', fontweight='bold')\n",
        "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Completed services\n",
        "    axes[0, 1].bar(policy_names, [results[p]['completed'] for p in policy_names], color=colors, alpha=0.7, edgecolor='black')\n",
        "    axes[0, 1].set_ylabel('Completed Services')\n",
        "    axes[0, 1].set_title('Completed Services', fontweight='bold')\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Blocking rate\n",
        "    axes[0, 2].bar(policy_names, [results[p]['blocking_rate'] for p in policy_names], color=colors, alpha=0.7, edgecolor='black')\n",
        "    axes[0, 2].set_ylabel('Blocking Rate (%)')\n",
        "    axes[0, 2].set_title('Blocking Rate', fontweight='bold')\n",
        "    axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Blocked patients\n",
        "    axes[1, 0].bar(policy_names, [results[p]['blocked'] for p in policy_names], color=colors, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 0].set_ylabel('Blocked Patients')\n",
        "    axes[1, 0].set_title('Blocked Patients', fontweight='bold')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Average queue size\n",
        "    axes[1, 1].bar(policy_names, [results[p]['avg_queue'] for p in policy_names], color=colors, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_ylabel('Average Queue Size')\n",
        "    axes[1, 1].set_title('Average Queue Size', fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Average staff\n",
        "    axes[1, 2].bar(policy_names, [results[p]['avg_staff'] for p in policy_names], color=colors, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 2].set_ylabel('Average Staff')\n",
        "    axes[1, 2].set_title('Average Staff Level', fontweight='bold')\n",
        "    axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results, agent\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# BLOCK 10: Main entry point and default parameters\n",
        "# Description:\n",
        "#   Script entry for running the full experiment: define base parameters,\n",
        "#   train and compare all policies, and (optionally) run a live visualization.\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters\n",
        "    params = {\n",
        "        'mu': 10.0,           # Arrival rate per hour\n",
        "        'lambda1': 0.5,       # Service rate for critical\n",
        "        'lambda2': 2.5,       # Service rate for urgent\n",
        "        'lambda3': 4.0,       # Service rate for non-urgent\n",
        "        'M': 20,              # Queue capacity\n",
        "        'Smax': 10,           # Max staff\n",
        "        'initial_staff': 2,   # Initial staff\n",
        "        'r1': 100,            # Reward for critical\n",
        "        'r2': 50,             # Reward for urgent\n",
        "        'r3': 20,             # Reward for non-urgent\n",
        "        'cm': 10,             # Maintenance cost per staff per hour\n",
        "        'ca': 50,             # Activation cost per staff\n",
        "        'cp': 100            # Blocking penalty per patient\n",
        "    }\n",
        "\n",
        "    # ========================================================================\n",
        "    # OPTION 1: Train and compare all policies\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SIMULATION-BASED REINFORCEMENT LEARNING\")\n",
        "    print(\"=\"*70)\n",
        "    results, trained_agent = compare_all_policies(params, sim_time=100)\n",
        "\n",
        "    # ========================================================================\n",
        "    # OPTION 2: Run live visual simulation with trained RL agent\n",
        "    # (commented out by default)\n",
        "    # ========================================================================\n",
        "    # print(\"\\n\" + \"=\"*70)\n",
        "    # print(\"LIVE VISUAL SIMULATION WITH RL POLICY\")\n",
        "    # print(\"=\"*70)\n",
        "    # print(\"\\nRunning visual simulation with trained RL agent...\")\n",
        "    # print(\"- Patients shown as colored dots\")\n",
        "    # print(\"- RL agent makes staffing decisions every hour\")\n",
        "    # print(\"=\"*70)\n",
        "\n",
        "    # For Jupyter notebook:\n",
        "    # %matplotlib notebook  # or %matplotlib widget\n",
        "    # anim, system = live_visual_simulation(params, sim_time=50, policy='rl', rl_agent=trained_agent)\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "    # print(\"\\n\" + \"=\"*70)\n",
        "    # print(\"SIMULATION COMPLETE\")\n",
        "    # print(\"=\"*70)\n",
        "    # print(f\"Final Statistics with RL Policy:\")\n",
        "    # print(f\"  Total Arrivals: {system.stats['arrivals']}\")\n",
        "    # if system.stats['arrivals'] > 0:\n",
        "    #     print(f\"  Total Blocked: {system.stats['blocked']} ({system.stats['blocked']/system.stats['arrivals']*100:.2f}%)\")\n",
        "    # else:\n",
        "    #     print(f\"  Total Blocked: {system.stats['blocked']} (N/A - no arrivals)\")\n",
        "    # print(f\"  Total Completed: {sum(system.stats['completed'].values())}\")\n",
        "    # print(f\"  Final Reward: {system.stats['total_reward']:.2f}\")\n",
        "    # print(f\"  Q-table size: {len(trained_agent.q_table)} states\")\n",
        "    # print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N26SbT5vb16A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AV-w3ElcT4ce"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdkOOiK4OHes"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}